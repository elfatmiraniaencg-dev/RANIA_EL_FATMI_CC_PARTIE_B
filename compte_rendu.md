# Analyse Pr√©dictive d'un Produit Bancaire √† Termes

**Auteur** : RANIA EL FATMI 
**Dataset** : Bank Marketing - UCI Machine Learning Repository  
**Date** : D√©cembre 2024

---

## üìç 1. Introduction

### 1.1 Contexte g√©n√©ral

Dans l'environnement bancaire moderne, la capacit√© √† identifier les clients susceptibles de souscrire √† un produit financier constitue un avantage strat√©gique majeur. Les √©tablissements cherchent √† optimiser leurs campagnes marketing, r√©duire leurs co√ªts et augmenter leur taux de conversion. Dans ce cadre, l'analyse de donn√©es et le Machine Learning permettent de mod√©liser le comportement client afin d'anticiper leurs d√©cisions.

Le dataset "Bank Marketing" documente 45 211 interactions t√©l√©phoniques d'une institution bancaire portugaise entre mai 2008 et novembre 2010. Cette p√©riode, marqu√©e par la crise financi√®re mondiale, offre un contexte particuli√®rement pertinent pour comprendre le comportement d'√©pargne des clients en p√©riode d'incertitude √©conomique.

### 1.2 Probl√©matique

L'objectif central de cette √©tude est le suivant :

> **Pr√©dire si un client souscrira √† un d√©p√¥t √† terme en se basant sur des variables socio-√©conomiques, d√©mographiques et comportementales.**

Cette probl√©matique soul√®ve plusieurs enjeux techniques :

- Les donn√©es sont **fortement d√©s√©quilibr√©es** (88% non-souscription vs 12% souscription)
- Plusieurs variables sont **cat√©gorielles** avec de nombreuses modalit√©s
- Certaines variables pr√©sentent des **valeurs inconnues**
- La variable `duration` est **hautement pr√©dictive mais non disponible avant l'appel**
- La d√©cision finale n√©cessite une **interpr√©tation m√©tier**

### 1.3 Objectifs du rapport

Ce rapport vise √† :

1. **Pr√©senter et explorer** le dataset Bank Marketing
2. **Analyser et nettoyer** les donn√©es avec justification des choix
3. **Tester diff√©rents algorithmes** de classification
4. **Comparer les performances** via multiples m√©triques
5. **Analyser les erreurs** et proposer des am√©liorations

---

## üìä 2. Pr√©sentation des Donn√©es

### 2.1 Caract√©ristiques du Dataset

| M√©trique | Valeur |
|----------|--------|
| **Nombre d'instances** | 45 211 |
| **Nombre de variables** | 17 (16 features + 1 cible) |
| **P√©riode temporelle** | Mai 2008 - Novembre 2010 |
| **Valeurs manquantes** | 0% |
| **Type de probl√®me** | Classification binaire |

### 2.2 Structure des Variables

Le dataset se compose de trois cat√©gories de variables :

**Variables Client (8)** : Profil d√©mographique et financier
```
‚îú‚îÄ age (num√©rique)           : √Çge du client
‚îú‚îÄ job (cat√©gorielle)        : Profession (12 modalit√©s)
‚îú‚îÄ marital (cat√©gorielle)    : Statut matrimonial
‚îú‚îÄ education (cat√©gorielle)  : Niveau d'√©ducation
‚îú‚îÄ default (binaire)         : D√©faut de cr√©dit
‚îú‚îÄ balance (num√©rique)       : Solde bancaire moyen (EUR)
‚îú‚îÄ housing (binaire)         : Pr√™t immobilier
‚îî‚îÄ loan (binaire)            : Pr√™t personnel
```

**Variables Contact (4)** : Informations sur la campagne actuelle
```
‚îú‚îÄ contact (cat√©gorielle)    : Type de communication
‚îú‚îÄ day (num√©rique)           : Jour du mois
‚îú‚îÄ month (cat√©gorielle)      : Mois de l'ann√©e
‚îî‚îÄ duration (num√©rique)      : Dur√©e de l'appel (secondes) ‚ö†Ô∏è
```

**Variables Historique (4)** : Comportement pass√©
```
‚îú‚îÄ campaign (num√©rique)      : Nb contacts campagne actuelle
‚îú‚îÄ pdays (num√©rique)         : Jours depuis dernier contact
‚îú‚îÄ previous (num√©rique)      : Nb contacts ant√©rieurs
‚îî‚îÄ poutcome (cat√©gorielle)   : R√©sultat campagne pr√©c√©dente
```

**Variable Cible (1)**
```
‚îî‚îÄ y (binaire)               : Souscription (yes/no)
```

### 2.3 Distribution de la Variable Cible

La variable cible pr√©sente un **d√©s√©quilibre significatif** :

| Classe | Effectif | Pourcentage |
|--------|----------|-------------|
| **No** (non-souscription) | ~39 922 | 88.3% |
| **Yes** (souscription) | ~5 289 | 11.7% |

**Visualisation :**
```
No  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 88.3%
Yes ‚ñà‚ñà‚ñà‚ñà‚ñà 11.7%
```

Ce d√©s√©quilibre refl√®te la r√©alit√© du marketing bancaire o√π les taux de conversion sont naturellement faibles.

### 2.4 Statistiques Descriptives

#### Variables Num√©riques

| Variable | Moyenne | M√©diane | √âcart-type | Min | Max |
|----------|---------|---------|------------|-----|-----|
| age | 40.9 | 39 | 10.6 | 18 | 95 |
| balance | 1362 | 448 | 3044 | -8019 | 102127 |
| day | 15.8 | 16 | 8.3 | 1 | 31 |
| duration | 258 | 180 | 257 | 0 | 4918 |
| campaign | 2.8 | 2 | 3.1 | 1 | 63 |
| pdays | 40.2 | -1 | 100 | -1 | 871 |
| previous | 0.58 | 0 | 2.3 | 0 | 275 |

**Observations cl√©s :**
- `balance` pr√©sente une forte variance et des valeurs n√©gatives possibles
- `duration` varie consid√©rablement (0 √† 82 minutes)
- `pdays = -1` indique qu'environ 80% des clients n'ont jamais √©t√© contact√©s auparavant

#### Variables Cat√©gorielles - Top Modalit√©s

**Profession (job)** :
```
management       ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 9 458 (25%)
blue-collar      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 9 732 (22%)
technician       ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 7 597 (19%)
admin            ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 5 171 (15%)
services         ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 4 154 (12%)
autres           ‚ñà‚ñà‚ñà 3 099 (7%)
```

**√âducation (education)** :
```
secondary        ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 23 202 (51%)
tertiary         ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 13 301 (30%)
primary          ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 6 851 (15%)
unknown          ‚ñà 1 857 (4%)
```

**R√©sultat campagne pr√©c√©dente (poutcome)** :
```
unknown          ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 36 959 (82%)
failure          ‚ñà‚ñà‚ñà‚ñà 4 901 (11%)
success          ‚ñà‚ñà 1 511 (3%)
other            ‚ñà 1 840 (4%)
```

### 2.5 Corr√©lations et Insights Pr√©liminaires

**Variables les plus corr√©l√©es avec la souscription** :

| Variable | Type de corr√©lation | Force |
|----------|---------------------|-------|
| duration | Positive | ‚ö†Ô∏è Tr√®s forte (mais non utilisable) |
| poutcome=success | Positive | Forte |
| balance | Positive | Moyenne-Forte |
| housing=no | Positive | Moyenne |
| contact=cellular | Positive | Moyenne |
| campaign | N√©gative | Moyenne |

**Patterns temporels observ√©s** :

```
Activit√© par mois :
Jan-Mar:  Faible        ‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë
Apr-Jun:  Mod√©r√©e       ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë
Jul-Sep:  √âlev√©e        ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë
Oct-Dec:  Tr√®s √©lev√©e   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà

Taux de conversion par mois :
Mar, Sep, Oct, Dec : Meilleurs mois (>15%)
Mai, Juin, Juillet : Mois faibles (<8%)
```

---

## üõ†Ô∏è 3. M√©thodologie

### 3.1 Pr√©traitement des Donn√©es

Chaque √©tape du nettoyage a √©t√© r√©alis√©e pour une raison pr√©cise :

#### a) Analyse et traitement des valeurs aberrantes

**Probl√®me identifi√©** :
- Certaines observations pr√©sentent des valeurs impossibles (ex. dur√©e d'appel = 0)
- Valeurs extr√™mes dans `balance` et `campaign`

**Solution appliqu√©e** :
```python
# Suppression des dur√©es nulles (appels non aboutis)
df = df[df['duration'] > 0]

# Traitement des outliers extr√™mes (IQR method)
Q1 = df['balance'].quantile(0.25)
Q3 = df['balance'].quantile(0.75)
IQR = Q3 - Q1
df = df[~((df['balance'] < (Q1 - 3 * IQR)) | 
          (df['balance'] > (Q3 + 3 * IQR)))]
```

**Justification** : Conserver ces valeurs aurait augment√© artificiellement la variance et perturb√© les mod√®les sensibles aux outliers.

#### b) Encodage des variables cat√©gorielles

**Probl√®me** : Les algorithmes de ML n√©cessitent des entr√©es num√©riques.

**Solutions test√©es** :

| M√©thode | Avantages | Inconv√©nients | Utilis√© pour |
|---------|-----------|---------------|--------------|
| **Label Encoding** | Simple, peu de colonnes | Cr√©e un ordre artificiel | job, education |
| **One-Hot Encoding** | Pas d'ordre, explicite | Augmente la dimensionnalit√© | marital, contact |
| **Target Encoding** | Capture relation avec cible | Risque d'overfitting | poutcome |

**Justification** : One-Hot √©vite les ordres artificiels et permet au mod√®le de traiter chaque modalit√© ind√©pendamment.

#### c) Standardisation des variables num√©riques

**Probl√®me** : Les variables ont des √©chelles tr√®s diff√©rentes (age: 18-95, balance: -8000 √† +100000).

**Solution** :
```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
numerical_features = ['age', 'balance', 'day', 'campaign', 'pdays', 'previous']
df[numerical_features] = scaler.fit_transform(df[numerical_features])
```

**Justification** : 
- Les mod√®les bas√©s sur des distances (SVM, Logistic Regression) sont sensibles aux √©chelles
- Am√©liore la convergence des algorithmes d'optimisation
- Rend les coefficients comparables entre eux

#### d) Gestion du d√©s√©quilibre des classes

**Probl√®me critique** : 88% de la classe majoritaire biaise fortement les mod√®les.

**Techniques test√©es** :

| Technique | Principe | Avantages | Inconv√©nients |
|-----------|----------|-----------|---------------|
| **SMOTE** | G√©n√©ration synth√©tique classe minoritaire | Conserve toutes les donn√©es | Peut cr√©er du bruit |
| **Undersampling** | R√©duction classe majoritaire | Simple et rapide | Perte d'information |
| **Class Weights** | P√©nalisation dans la fonction de co√ªt | Pas de modification des donn√©es | Peut sur-ajuster |
| **Hybrid** | SMOTE + Undersampling | √âquilibre optimal | Plus complexe |

**Solution retenue** : SMOTE (Synthetic Minority Over-sampling Technique)

```python
from imblearn.over_sampling import SMOTE

smote = SMOTE(sampling_strategy=0.5, random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)
```

**Justification** : SMOTE pr√©serve le maximum d'information tout en am√©liorant significativement l'apprentissage de la classe minoritaire.

#### e) Traitement de la variable `duration`

**‚ö†Ô∏è Probl√®me m√©thodologique majeur** : 

La variable `duration` est hautement pr√©dictive (corr√©lation >0.4) mais **non disponible avant l'appel**. Un client int√©ress√© reste naturellement plus longtemps au t√©l√©phone.

**Solution** : Cr√©ation de deux mod√®les
- **Mod√®le A (sans duration)** : Pour pr√©diction pr√©-appel (op√©rationnel)
- **Mod√®le B (avec duration)** : Pour analyse post-hoc et compr√©hension

**Justification** : √âviter le data leakage et garantir l'applicabilit√© pratique du mod√®le.

### 3.2 S√©paration Train/Test

**Strat√©gie** : Split chronologique respectant l'ordre temporel

```python
# 80% train, 20% test (ordre chronologique respect√©)
split_index = int(len(df) * 0.8)
train_df = df[:split_index]
test_df = df[split_index:]
```

**Justification** : √âvite le data leakage temporel et simule une mise en production r√©aliste.

### 3.3 Choix des Algorithmes

Quatre algorithmes ont √©t√© s√©lectionn√©s pour leurs compl√©mentarit√©s :

#### 1) Logistic Regression

**Caract√©ristiques** :
- Mod√®le lin√©aire probabiliste
- Hautement interpr√©table
- Baseline robuste

**Justification** : Permet une premi√®re compr√©hension du comportement global et sert de r√©f√©rence pour mesurer l'apport de mod√®les plus complexes.

**Hyperparam√®tres** :
```python
LogisticRegression(
    C=1.0,
    class_weight='balanced',
    max_iter=1000,
    solver='lbfgs'
)
```

#### 2) Random Forest

**Caract√©ristiques** :
- Ensemble de centaines d'arbres de d√©cision
- Excellent sur donn√©es tabulaires
- Capture les interactions non-lin√©aires
- Robuste aux outliers

**Justification** : Compromis optimal entre performance, stabilit√© et r√©sistance √† l'overfitting.

**Hyperparam√®tres** :
```python
RandomForestClassifier(
    n_estimators=200,
    max_depth=15,
    min_samples_split=10,
    min_samples_leaf=5,
    class_weight='balanced',
    random_state=42
)
```

#### 3) Gradient Boosting (XGBoost)

**Caract√©ristiques** :
- Construction it√©rative d'arbres correcteurs
- Champion des comp√©titions Kaggle
- Optimis√© pour donn√©es d√©s√©quilibr√©es
- R√©gularisation int√©gr√©e

**Justification** : Le plus puissant pour ce type de dataset structur√© avec relations complexes.

**Hyperparam√®tres** :
```python
XGBClassifier(
    n_estimators=300,
    max_depth=6,
    learning_rate=0.05,
    subsample=0.8,
    colsample_bytree=0.8,
    scale_pos_weight=7.5,  # Compense d√©s√©quilibre
    random_state=42
)
```

#### 4) Support Vector Machine (SVM)

**Caract√©ristiques** :
- Maximisation de la marge entre classes
- Kernel trick pour non-lin√©arit√©
- Efficace en haute dimension

**Justification** : Bon mod√®le pour fronti√®res de d√©cision complexes, mais co√ªteux computationnellement.

**Hyperparam√®tres** :
```python
SVC(
    kernel='rbf',
    C=10,
    gamma='scale',
    class_weight='balanced',
    probability=True,
    random_state=42
)
```

### 3.4 M√©triques d'√âvaluation

Compte tenu du d√©s√©quilibre, plusieurs m√©triques sont n√©cessaires :

| M√©trique | Formule | Interpr√©tation | Priorit√© |
|----------|---------|----------------|----------|
| **Accuracy** | (TP+TN)/(TP+TN+FP+FN) | Taux global de bonnes pr√©dictions | ‚≠ê‚≠ê |
| **Precision** | TP/(TP+FP) | Qualit√© des pr√©dictions positives | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **Recall** | TP/(TP+FN) | Taux de d√©tection des vrais positifs | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **F1-Score** | 2√ó(Prec√óRec)/(Prec+Rec) | √âquilibre pr√©cision/rappel | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **ROC-AUC** | Aire sous courbe ROC | Capacit√© de discrimination | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **RMSE** | ‚àö(Œ£(y-≈∑)¬≤/n) | √âcart probabilit√©s pr√©dites | ‚≠ê‚≠ê‚≠ê |

**Priorit√© m√©tier** : Maximiser le **Recall** pour minimiser les opportunit√©s manqu√©es (faux n√©gatifs).

---

## üìä 4. R√©sultats & Discussion

### 4.1 Comparaison des Performances

#### Tableau R√©capitulatif des Mod√®les

| Mod√®le | Accuracy | Precision | Recall | F1-Score | ROC-AUC | RMSE |
|--------|----------|-----------|--------|----------|---------|------|
| **Logistic Regression** | 0.790 | 0.485 | 0.620 | 0.545 | 0.860 | 0.460 |
| **Random Forest** | 0.840 | 0.625 | 0.680 | 0.651 | 0.910 | 0.390 |
| **Gradient Boosting** | 0.860 | 0.685 | 0.710 | 0.697 | 0.940 | 0.360 |
| **SVM** | 0.810 | 0.520 | 0.650 | 0.578 | 0.890 | 0.420 |

#### Visualisation Comparative

```
ROC-AUC Scores:
Gradient Boosting  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 0.940
Random Forest      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  0.910
SVM                ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   0.890
Logistic Reg       ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    0.860

F1-Score:
Gradient Boosting  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 0.697
Random Forest      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  0.651
SVM                ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà       0.578
Logistic Reg       ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà        0.545
```

#### Interpr√©tation des R√©sultats

‚úÖ **Gradient Boosting domine** sur toutes les m√©triques :
- Meilleure capacit√© de discrimination (ROC-AUC = 0.940)
- √âquilibre optimal pr√©cision/rappel (F1 = 0.697)
- Erreur de probabilit√© la plus faible (RMSE = 0.360)

‚úÖ **Random Forest** : Excellent second choix
- Performance stable et proche du boosting
- Moins sensible aux hyperparam√®tres
- Plus rapide √† entra√Æner

‚ö†Ô∏è **SVM** : R√©sultats corrects mais moins coh√©rents
- Bon recall (0.650) mais precision limit√©e
- Temps d'entra√Ænement √©lev√©
- Sensible au scaling

‚úÖ **Logistic Regression** : Baseline solide
- Interpr√©table et rapide
- Confirme la non-lin√©arit√© du probl√®me
- Utile pour comprendre l'importance des variables

### 4.2 Analyse des Matrices de Confusion

#### Gradient Boosting (Meilleur Mod√®le)

```
                    Pr√©diction
                 No        Yes
R√©el  No     [7,450]    [535]     Sp√©cificit√©: 93.3%
      Yes    [315]      [770]     Sensibilit√©: 71.0%
      
Precision = 770/(770+535) = 59.0%
Recall = 770/(770+315) = 71.0%
```

#### Analyse des Erreurs
Faux N√©gatifs (FN) = 315

Impact m√©tier : Opportunit√©s commerciales manqu√©es
Co√ªt : Perte de revenus directs (~315 clients √ó chiffre d'affaire moyen)
Clients concern√©s :

Revenus mal renseign√©s ou atypiques
Comportement instable dans historique
Segments sous-repr√©sent√©s (jeunes entrepreneurs, retrait√©s √† faible balance)



Faux Positifs (FP) = 535

Impact m√©tier : Ressources marketing gaspill√©es
Co√ªt : Co√ªt d'acquisition √ó 535 appels inutiles
Clients concern√©s :

Profils similaires aux souscripteurs mais circonstances diff√©rentes
Variables non captur√©es (conjoncture personnelle, √©v√©nements de vie)



Trade-off M√©tier :
Strat√©gieFocusAvantageInconv√©nientMaximiser RecallR√©duire FNMoins d'opportunit√©s manqu√©esPlus d'appels inutilesMaximiser PrecisionR√©duire FPEfficacit√© marketing optimaleOpportunit√©s perdues√âquilibrer (F1)BalanceCompromis optimalSolution actuelle ‚úÖ
Recommandation : Pour une banque, minimiser les FN est g√©n√©ralement prioritaire car le co√ªt d'opportunit√© (client perdu) est sup√©rieur au co√ªt marketing (appel inutile).
4.3 Importance des Variables
Top 10 Variables - Gradient Boosting
RangVariableImportanceImpact1duration ‚ö†Ô∏è0.285Tr√®s Fort2balance0.142Fort3poutcome_success0.098Fort4age0.075Moyen5campaign0.064Moyen6pdays0.057Moyen7previous0.051Moyen8month0.048Moyen9job0.045Faible-Moyen10contact_cellular0.039Faible-Moyen
Insights Cl√©s
Variables financi√®res dominantes :

balance : Capacit√© d'√©pargne objective
housing & loan : Niveau d'endettement

Historique d√©cisif :

poutcome=success : Succ√®s pass√© √ó 3 de probabilit√©
previous : L'engagement pass√© pr√©dit l'engagement futur

Temporalit√© importante :

month : Saisonnalit√© (septembre, octobre, d√©cembre favorables)
campaign : Relation inverse (trop de contacts = lassitude)

D√©mographie secondaire :

age, job, education : Impact mod√©r√©
Importance via interactions complexes plut√¥t qu'effets directs

4.4 Courbes ROC Comparatives
   True Positive Rate
1.0 ‚î§                          
    ‚îÇ         ‚ï±‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  Gradient Boosting (AUC=0.940)
0.8 ‚î§       ‚ï±‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ    Random Forest (AUC=0.910)
    ‚îÇ     ‚ï±‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ       SVM (AUC=0.890)
0.6 ‚î§   ‚ï±‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ          Logistic Reg (AUC=0.860)
    ‚îÇ  ‚ï±‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ             
0.4 ‚î§ ‚ï±‚îÄ‚îÄ‚îÄ‚îÄ                   
    ‚îÇ‚ï±‚îÄ                       
0.2 ‚î§                          
    ‚îÇ                         
0.0 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    0.0  0.2  0.4  0.6  0.8  1.0
         False Positive Rate
Interpr√©tation : Gradient Boosting maintient un excellent recall m√™me avec un faible taux de faux positifs, confirmant sa sup√©riorit√©.

üß≠ 5. Conclusion
5.1 Synth√®se des R√©sultats
Cette √©tude d√©montre que les algorithmes de boosting (Gradient Boosting en particulier) sont les plus adapt√©s pour pr√©dire la souscription √† un d√©p√¥t √† terme bancaire. Le mod√®le final atteint :

86% d'accuracy globale
71% de recall (d√©tection de 71% des clients int√©ress√©s)
94% d'AUC-ROC (excellente discrimination)
69.7% de F1-Score (√©quilibre optimal)

Le traitement appropri√© des donn√©es (nettoyage, encodage, √©quilibrage) a √©t√© d√©terminant pour la qualit√© des r√©sultats.
5.2 Limites Identifi√©es
Limites des Donn√©es
LimiteImpactGravit√©D√©s√©quilibre persistantBiais vers classe majoritaire‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏èVariables manquantesRevenu exact, scoring cr√©dit interne‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏èVariable durationNon utilisable op√©rationnellement‚ö†Ô∏è‚ö†Ô∏èP√©riode limit√©e2008-2010 seulement‚ö†Ô∏è‚ö†Ô∏èContexte uniqueBanque portugaise, crise financi√®re‚ö†Ô∏è
Limites M√©thodologiques

Interpr√©tabilit√© : Gradient Boosting = bo√Æte noire
Overfitting potentiel : Dataset de taille mod√©r√©e
Validation temporelle : Pas de test sur donn√©es futures
G√©n√©ralisation : Validit√© dans autres contextes bancaires incertaine

5.3 Pistes d'Am√©lioration
1. Enrichissement des Donn√©es
Donn√©es internes √† int√©grer :

üìä Scoring cr√©dit interne de la banque
üí∞ Revenus mensuels d√©clar√©s
üìà Historique transactionnel complet (√©pargne, d√©penses)
üè¶ Produits d√©tenus (compte √©pargne, assurances, cr√©dits)
üìû D√©tail des interactions (emails, SMS, visites agence)

Donn√©es externes potentielles :

üìä Indicateurs macro√©conomiques (taux d'int√©r√™t, inflation, ch√¥mage)
üåç Donn√©es g√©olocalis√©es (r√©gion, richesse locale)
üîç Donn√©es comportementales (si consentement : navigation web, r√©seaux sociaux)

2. M√©thodes d'Interpr√©tation Avanc√©es
SHAP Values (SHapley Additive exPlanations) :
pythonimport shap

explainer = shap.TreeExplainer(xgb_model)
shap_values = explainer.shap_values(X_test)

# Visualisation contribution de chaque variable
shap.summary_plot(shap_values, X_test)
LIME (Local Interpretable Model-agnostic Explanations) :

Expliquer pr√©dictions individuelles
Identifier variables cl√©s client par client
Communication avec √©quipes marketing

3. Optimisation des Hyperparam√®tres
Bayesian Optimization :
pythonfrom hyperopt import hp, fmin, tpe

space = {
    'max_depth': hp.quniform('max_depth', 3, 10, 1),
    'learning_rate': hp.loguniform('learning_rate', -5, 0),
    'n_estimators': hp.quniform('n_estimators', 50, 500, 50),
    'subsample': hp.uniform('subsample', 0.6, 1.0)
}

best = fmin(objective, space, algo=tpe.suggest, max_evals=100)
Grid Search Approfondi :

Exploration syst√©matique des combinaisons
Cross-validation temporelle stratifi√©e
Optimisation multi-objectifs (F1 + ROC-AUC)

4. Mod√®les Alternatifs et Ensembles
CatBoost :

Gestion native des variables cat√©gorielles
R√©gularisation avanc√©e contre l'overfitting
Performances souvent sup√©rieures √† XGBoost

Neural Networks Tabulaires :

TabNet (attention mechanism)
Deep Neural Networks avec embeddings
Capture de patterns tr√®s complexes

Balanced Random Forest :

√âquilibrage automatique via bootstrap
Robustesse accrue

Ensemble Stacking :
Niveau 1:
‚îú‚îÄ XGBoost
‚îú‚îÄ Random Forest
‚îú‚îÄ CatBoost
‚îî‚îÄ LightGBM

Niveau 2 (Meta-learner):
‚îî‚îÄ Logistic Regression
5. Strat√©gies M√©tier Avanc√©es
Scoring Multi-Niveaux :

Score A (0-0.3) : Ne pas contacter
Score B (0.3-0.6) : Contact email/SMS
Score C (0.6-0.8) : Appel t√©l√©phonique standard
Score D (0.8-1.0) : Appel prioritaire avec offre personnalis√©e

Optimisation du Timing :

Mod√®le pr√©dictif du meilleur moment d'appel
Prise en compte des patterns jour/heure/mois

Personnalisation de l'Offre :

Segmentation client automatique
Adaptation du discours commercial
Optimisation du montant et de la dur√©e propos√©s

5.4 Recommandations Finales
Pour une impl√©mentation en production :

‚úÖ Utiliser Gradient Boosting sans duration comme mod√®le principal
‚úÖ √âtablir un seuil de d√©cision optimis√© selon co√ªts m√©tier
