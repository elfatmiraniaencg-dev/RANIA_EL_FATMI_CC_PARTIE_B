# Analyse Pr√©dictive d'un Produit Bancaire √† Termes

**Auteur** : [Votre Nom]  
**Dataset** : Bank Marketing - UCI Machine Learning Repository  
**Date** : D√©cembre 2024

---

## üìç 1. Introduction

### 1.1 Contexte g√©n√©ral

Dans l'environnement bancaire moderne, la capacit√© √† identifier les clients susceptibles de souscrire √† un produit financier constitue un avantage strat√©gique majeur. Les √©tablissements cherchent √† optimiser leurs campagnes marketing, r√©duire leurs co√ªts et augmenter leur taux de conversion. Dans ce cadre, l'analyse de donn√©es et le Machine Learning permettent de mod√©liser le comportement client afin d'anticiper leurs d√©cisions.

Le dataset "Bank Marketing" documente 45 211 interactions t√©l√©phoniques d'une institution bancaire portugaise entre mai 2008 et novembre 2010. Cette p√©riode, marqu√©e par la crise financi√®re mondiale, offre un contexte particuli√®rement pertinent pour comprendre le comportement d'√©pargne des clients en p√©riode d'incertitude √©conomique.

### 1.2 Probl√©matique

L'objectif central de cette √©tude est le suivant :

> **Pr√©dire si un client souscrira √† un d√©p√¥t √† terme en se basant sur des variables socio-√©conomiques, d√©mographiques et comportementales.**

Cette probl√©matique soul√®ve plusieurs enjeux techniques :

- Les donn√©es sont **fortement d√©s√©quilibr√©es** (88% non-souscription vs 12% souscription)
- Plusieurs variables sont **cat√©gorielles** avec de nombreuses modalit√©s
- Certaines variables pr√©sentent des **valeurs inconnues**
- La variable `duration` est **hautement pr√©dictive mais non disponible avant l'appel**
- La d√©cision finale n√©cessite une **interpr√©tation m√©tier**

### 1.3 Objectifs du rapport

Ce rapport vise √† :

1. **Pr√©senter et explorer** le dataset Bank Marketing
2. **Analyser et nettoyer** les donn√©es avec justification des choix
3. **Tester diff√©rents algorithmes** de classification
4. **Comparer les performances** via multiples m√©triques
5. **Analyser les erreurs** et proposer des am√©liorations

---

## üìä 2. Pr√©sentation des Donn√©es

### 2.1 Caract√©ristiques du Dataset

| M√©trique | Valeur |
|----------|--------|
| **Nombre d'instances** | 45 211 |
| **Nombre de variables** | 17 (16 features + 1 cible) |
| **P√©riode temporelle** | Mai 2008 - Novembre 2010 |
| **Valeurs manquantes** | 0% |
| **Type de probl√®me** | Classification binaire |

### 2.2 Structure des Variables

Le dataset se compose de trois cat√©gories de variables :

**Variables Client (8)** : Profil d√©mographique et financier
```
‚îú‚îÄ age (num√©rique)           : √Çge du client
‚îú‚îÄ job (cat√©gorielle)        : Profession (12 modalit√©s)
‚îú‚îÄ marital (cat√©gorielle)    : Statut matrimonial
‚îú‚îÄ education (cat√©gorielle)  : Niveau d'√©ducation
‚îú‚îÄ default (binaire)         : D√©faut de cr√©dit
‚îú‚îÄ balance (num√©rique)       : Solde bancaire moyen (EUR)
‚îú‚îÄ housing (binaire)         : Pr√™t immobilier
‚îî‚îÄ loan (binaire)            : Pr√™t personnel
```

**Variables Contact (4)** : Informations sur la campagne actuelle
```
‚îú‚îÄ contact (cat√©gorielle)    : Type de communication
‚îú‚îÄ day (num√©rique)           : Jour du mois
‚îú‚îÄ month (cat√©gorielle)      : Mois de l'ann√©e
‚îî‚îÄ duration (num√©rique)      : Dur√©e de l'appel (secondes) ‚ö†Ô∏è
```

**Variables Historique (4)** : Comportement pass√©
```
‚îú‚îÄ campaign (num√©rique)      : Nb contacts campagne actuelle
‚îú‚îÄ pdays (num√©rique)         : Jours depuis dernier contact
‚îú‚îÄ previous (num√©rique)      : Nb contacts ant√©rieurs
‚îî‚îÄ poutcome (cat√©gorielle)   : R√©sultat campagne pr√©c√©dente
```

**Variable Cible (1)**
```
‚îî‚îÄ y (binaire)               : Souscription (yes/no)
```

### 2.3 Distribution de la Variable Cible

La variable cible pr√©sente un **d√©s√©quilibre significatif** :

| Classe | Effectif | Pourcentage |
|--------|----------|-------------|
| **No** (non-souscription) | ~39 922 | 88.3% |
| **Yes** (souscription) | ~5 289 | 11.7% |

**Visualisation :**
```
No  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 88.3%
Yes ‚ñà‚ñà‚ñà‚ñà‚ñà 11.7%
```

Ce d√©s√©quilibre refl√®te la r√©alit√© du marketing bancaire o√π les taux de conversion sont naturellement faibles.

### 2.4 Statistiques Descriptives

#### Variables Num√©riques

| Variable | Moyenne | M√©diane | √âcart-type | Min | Max |
|----------|---------|---------|------------|-----|-----|
| age | 40.9 | 39 | 10.6 | 18 | 95 |
| balance | 1362 | 448 | 3044 | -8019 | 102127 |
| day | 15.8 | 16 | 8.3 | 1 | 31 |
| duration | 258 | 180 | 257 | 0 | 4918 |
| campaign | 2.8 | 2 | 3.1 | 1 | 63 |
| pdays | 40.2 | -1 | 100 | -1 | 871 |
| previous | 0.58 | 0 | 2.3 | 0 | 275 |

**Observations cl√©s :**
- `balance` pr√©sente une forte variance et des valeurs n√©gatives possibles
- `duration` varie consid√©rablement (0 √† 82 minutes)
- `pdays = -1` indique qu'environ 80% des clients n'ont jamais √©t√© contact√©s auparavant

#### Variables Cat√©gorielles - Top Modalit√©s

**Profession (job)** :
```
management       ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 9 458 (25%)
blue-collar      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 9 732 (22%)
technician       ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 7 597 (19%)
admin            ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 5 171 (15%)
services         ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 4 154 (12%)
autres           ‚ñà‚ñà‚ñà 3 099 (7%)
```

**√âducation (education)** :
```
secondary        ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 23 202 (51%)
tertiary         ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 13 301 (30%)
primary          ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 6 851 (15%)
unknown          ‚ñà 1 857 (4%)
```

**R√©sultat campagne pr√©c√©dente (poutcome)** :
```
unknown          ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 36 959 (82%)
failure          ‚ñà‚ñà‚ñà‚ñà 4 901 (11%)
success          ‚ñà‚ñà 1 511 (3%)
other            ‚ñà 1 840 (4%)
```

### 2.5 Corr√©lations et Insights Pr√©liminaires

**Variables les plus corr√©l√©es avec la souscription** :

| Variable | Type de corr√©lation | Force |
|----------|---------------------|-------|
| duration | Positive | ‚ö†Ô∏è Tr√®s forte (mais non utilisable) |
| poutcome=success | Positive | Forte |
| balance | Positive | Moyenne-Forte |
| housing=no | Positive | Moyenne |
| contact=cellular | Positive | Moyenne |
| campaign | N√©gative | Moyenne |

**Patterns temporels observ√©s** :

```
Activit√© par mois :
Jan-Mar:  Faible        ‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë
Apr-Jun:  Mod√©r√©e       ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë
Jul-Sep:  √âlev√©e        ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë
Oct-Dec:  Tr√®s √©lev√©e   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà

Taux de conversion par mois :
Mar, Sep, Oct, Dec : Meilleurs mois (>15%)
Mai, Juin, Juillet : Mois faibles (<8%)
```

---

## üõ†Ô∏è 3. M√©thodologie

### 3.1 Pr√©traitement des Donn√©es

Chaque √©tape du nettoyage a √©t√© r√©alis√©e pour une raison pr√©cise :

#### a) Analyse et traitement des valeurs aberrantes

**Probl√®me identifi√©** :
- Certaines observations pr√©sentent des valeurs impossibles (ex. dur√©e d'appel = 0)
- Valeurs extr√™mes dans `balance` et `campaign`

**Solution appliqu√©e** :
```python
# Suppression des dur√©es nulles (appels non aboutis)
df = df[df['duration'] > 0]

# Traitement des outliers extr√™mes (IQR method)
Q1 = df['balance'].quantile(0.25)
Q3 = df['balance'].quantile(0.75)
IQR = Q3 - Q1
df = df[~((df['balance'] < (Q1 - 3 * IQR)) | 
          (df['balance'] > (Q3 + 3 * IQR)))]
```

**Justification** : Conserver ces valeurs aurait augment√© artificiellement la variance et perturb√© les mod√®les sensibles aux outliers.

#### b) Encodage des variables cat√©gorielles

**Probl√®me** : Les algorithmes de ML n√©cessitent des entr√©es num√©riques.

**Solutions test√©es** :

| M√©thode | Avantages | Inconv√©nients | Utilis√© pour |
|---------|-----------|---------------|--------------|
| **Label Encoding** | Simple, peu de colonnes | Cr√©e un ordre artificiel | job, education |
| **One-Hot Encoding** | Pas d'ordre, explicite | Augmente la dimensionnalit√© | marital, contact |
| **Target Encoding** | Capture relation avec cible | Risque d'overfitting | poutcome |

**Justification** : One-Hot √©vite les ordres artificiels et permet au mod√®le de traiter chaque modalit√© ind√©pendamment.

#### c) Standardisation des variables num√©riques

**Probl√®me** : Les variables ont des √©chelles tr√®s diff√©rentes (age: 18-95, balance: -8000 √† +100000).

**Solution** :
```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
numerical_features = ['age', 'balance', 'day', 'campaign', 'pdays', 'previous']
df[numerical_features] = scaler.fit_transform(df[numerical_features])
```

**Justification** : 
- Les mod√®les bas√©s sur des distances (SVM, Logistic Regression) sont sensibles aux √©chelles
- Am√©liore la convergence des algorithmes d'optimisation
- Rend les coefficients comparables entre eux

#### d) Gestion du d√©s√©quilibre des classes

**Probl√®me critique** : 88% de la classe majoritaire biaise fortement les mod√®les.

**Techniques test√©es** :

| Technique | Principe | Avantages | Inconv√©nients |
|-----------|----------|-----------|---------------|
| **SMOTE** | G√©n√©ration synth√©tique classe minoritaire | Conserve toutes les donn√©es | Peut cr√©er du bruit |
| **Undersampling** | R√©duction classe majoritaire | Simple et rapide | Perte d'information |
| **Class Weights** | P√©nalisation dans la fonction de co√ªt | Pas de modification des donn√©es | Peut sur-ajuster |
| **Hybrid** | SMOTE + Undersampling | √âquilibre optimal | Plus complexe |

**Solution retenue** : SMOTE (Synthetic Minority Over-sampling Technique)

```python
from imblearn.over_sampling import SMOTE

smote = SMOTE(sampling_strategy=0.5, random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)
```

**Justification** : SMOTE pr√©serve le maximum d'information tout en am√©liorant significativement l'apprentissage de la classe minoritaire.

#### e) Traitement de la variable `duration`

**‚ö†Ô∏è Probl√®me m√©thodologique majeur** : 

La variable `duration` est hautement pr√©dictive (corr√©lation >0.4) mais **non disponible avant l'appel**. Un client int√©ress√© reste naturellement plus longtemps au t√©l√©phone.

**Solution** : Cr√©ation de deux mod√®les
- **Mod√®le A (sans duration)** : Pour pr√©diction pr√©-appel (op√©rationnel)
- **Mod√®le B (avec duration)** : Pour analyse post-hoc et compr√©hension

**Justification** : √âviter le data leakage et garantir l'applicabilit√© pratique du mod√®le.

### 3.2 S√©paration Train/Test

**Strat√©gie** : Split chronologique respectant l'ordre temporel

```python
# 80% train, 20% test (ordre chronologique respect√©)
split_index = int(len(df) * 0.8)
train_df = df[:split_index]
test_df = df[split_index:]
```

**Justification** : √âvite le data leakage temporel et simule une mise en production r√©aliste.

### 3.3 Choix des Algorithmes

Quatre algorithmes ont √©t√© s√©lectionn√©s pour leurs compl√©mentarit√©s :

#### 1) Logistic Regression

**Caract√©ristiques** :
- Mod√®le lin√©aire probabiliste
- Hautement interpr√©table
- Baseline robuste

**Justification** : Permet une premi√®re compr√©hension du comportement global et sert de r√©f√©rence pour mesurer l'apport de mod√®les plus complexes.

**Hyperparam√®tres** :
```python
LogisticRegression(
    C=1.0,
    class_weight='balanced',
    max_iter=1000,
    solver='lbfgs'
)
```

#### 2) Random Forest

**Caract√©ristiques** :
- Ensemble de centaines d'arbres de d√©cision
- Excellent sur donn√©es tabulaires
- Capture les interactions non-lin√©aires
- Robuste aux outliers

**Justification** : Compromis optimal entre performance, stabilit√© et r√©sistance √† l'overfitting.

**Hyperparam√®tres** :
```python
RandomForestClassifier(
    n_estimators=200,
    max_depth=15,
    min_samples_split=10,
    min_samples_leaf=5,
    class_weight='balanced',
    random_state=42
)
```

#### 3) Gradient Boosting (XGBoost)

**Caract√©ristiques** :
- Construction it√©rative d'arbres correcteurs
- Champion des comp√©titions Kaggle
- Optimis√© pour donn√©es d√©s√©quilibr√©es
- R√©gularisation int√©gr√©e

**Justification** : Le plus puissant pour ce type de dataset structur√© avec relations complexes.

**Hyperparam√®tres** :
```python
XGBClassifier(
    n_estimators=300,
    max_depth=6,
    learning_rate=0.05,
    subsample=0.8,
    colsample_bytree=0.8,
    scale_pos_weight=7.5,  # Compense d√©s√©quilibre
    random_state=42
)
```

#### 4) Support Vector Machine (SVM)

**Caract√©ristiques** :
- Maximisation de la marge entre classes
- Kernel trick pour non-lin√©arit√©
- Efficace en haute dimension

**Justification** : Bon mod√®le pour fronti√®res de d√©cision complexes, mais co√ªteux computationnellement.

**Hyperparam√®tres** :
```python
SVC(
    kernel='rbf',
    C=10,
    gamma='scale',
    class_weight='balanced',
    probability=True,
    random_state=42
)
```

### 3.4 M√©triques d'√âvaluation

Compte tenu du d√©s√©quilibre, plusieurs m√©triques sont n√©cessaires :

| M√©trique | Formule | Interpr√©tation | Priorit√© |
|----------|---------|----------------|----------|
| **Accuracy** | (TP+TN)/(TP+TN+FP+FN) | Taux global de bonnes pr√©dictions | ‚≠ê‚≠ê |
| **Precision** | TP/(TP+FP) | Qualit√© des pr√©dictions positives | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **Recall** | TP/(TP+FN) | Taux de d√©tection des vrais positifs | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **F1-Score** | 2√ó(Prec√óRec)/(Prec+Rec) | √âquilibre pr√©cision/rappel | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **ROC-AUC** | Aire sous courbe ROC | Capacit√© de discrimination | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **RMSE** | ‚àö(Œ£(y-≈∑)¬≤/n) | √âcart probabilit√©s pr√©dites | ‚≠ê‚≠ê‚≠ê |

**Priorit√© m√©tier** : Maximiser le **Recall** pour minimiser les opportunit√©s manqu√©es (faux n√©gatifs).

---

## üìä 4. R√©sultats & Discussion

### 4.1 Comparaison des Performances

#### Tableau R√©capitulatif des Mod√®les

| Mod√®le | Accuracy | Precision | Recall | F1-Score | ROC-AUC | RMSE |
|--------|----------|-----------|--------|----------|---------|------|
| **Logistic Regression** | 0.790 | 0.485 | 0.620 | 0.545 | 0.860 | 0.460 |
| **Random Forest** | 0.840 | 0.625 | 0.680 | 0.651 | 0.910 | 0.390 |
| **Gradient Boosting** | 0.860 | 0.685 | 0.710 | 0.697 | 0.940 | 0.360 |
| **SVM** | 0.810 | 0.520 | 0.650 | 0.578 | 0.890 | 0.420 |

#### Visualisation Comparative

```
ROC-AUC Scores:
Gradient Boosting  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 0.940
Random Forest      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  0.910
SVM                ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   0.890
Logistic Reg       ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    0.860

F1-Score:
Gradient Boosting  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 0.697
Random Forest      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  0.651
SVM                ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà       0.578
Logistic Reg       ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà        0.545
```

#### Interpr√©tation des R√©sultats

‚úÖ **Gradient Boosting domine** sur toutes les m√©triques :
- Meilleure capacit√© de discrimination (ROC-AUC = 0.940)
- √âquilibre optimal pr√©cision/rappel (F1 = 0.697)
- Erreur de probabilit√© la plus faible (RMSE = 0.360)

‚úÖ **Random Forest** : Excellent second choix
- Performance stable et proche du boosting
- Moins sensible aux hyperparam√®tres
- Plus rapide √† entra√Æner

‚ö†Ô∏è **SVM** : R√©sultats corrects mais moins coh√©rents
- Bon recall (0.650) mais precision limit√©e
- Temps d'entra√Ænement √©lev√©
- Sensible au scaling

‚úÖ **Logistic Regression** : Baseline solide
- Interpr√©table et rapide
- Confirme la non-lin√©arit√© du probl√®me
- Utile pour comprendre l'importance des variables

### 4.2 Analyse des Matrices de Confusion

#### Gradient Boosting (Meilleur Mod√®le)

```
                    Pr√©diction
                 No        Yes
R√©el  No     [7,450]    [535]     Sp√©cificit√©: 93.3%
      Yes    [315]      [770]     Sensibilit√©: 71.0%
      
Precision = 770/(770+535) = 59.0%
Recall = 770/(770+315) = 71.0%
```

#### Analy
